{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba49fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Original', 'Source', 'Specialty'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取你的主表\n",
    "df = pd.read_csv(\"data/complete_reports.csv\")\n",
    "\n",
    "# 随机采样1000条（不放回，保证唯一）\n",
    "df_sample = df.sample(n=1000, random_state=42).copy()  # random_state 保证可复现\n",
    "df_sample = df_sample.reset_index(drop=True)   # <--- 让index从0开始\n",
    "\n",
    "# 确认有 'Original' 列\n",
    "print(df_sample.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8983d",
   "metadata": {},
   "source": [
    "Paraphrased\n",
    "\n",
    "Clinically Significant\n",
    "\n",
    "Noisy/Ambiguous\n",
    "\n",
    "Neutral Distractor\n",
    "\n",
    "Omission\n",
    "\n",
    "Hallucinated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b4ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt to generate the synthetic reports\n",
    "import random\n",
    "\n",
    "INSTRUCTIONS = {\n",
    "    \"Paraphrased\":\n",
    "        \"Paraphrase the following medical report without changing its clinical meaning:\",\n",
    "    \"Clinically Significant\":\n",
    "        \"Modify the following report to introduce a clinically significant change (for example, reverse or alter a key finding):\",\n",
    "    \"Noisy\":\n",
    "        \"Add vague, ambiguous, or contradictory statements to the following medical report, while keeping the main findings:\",\n",
    "    \"Neutral\":\n",
    "        \"Add a sentence of irrelevant but harmless information to the following medical report, without changing the clinical findings:\",\n",
    "}\n",
    "\n",
    "def script_omission(text):\n",
    "    sents = text.split('. ')\n",
    "    if len(sents) > 2:\n",
    "        omit = random.randint(0, len(sents)-1)\n",
    "        sents.pop(omit)\n",
    "        return '. '.join(sents)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def script_hallucinated(text):\n",
    "    fake_facts = [\n",
    "        \"A previously undocumented mass was detected in the left lung.\",\n",
    "        \"Imaging reveals possible early-stage sarcoidosis.\",\n",
    "        \"An unexpected metabolic anomaly is noted.\",\n",
    "        \"No radiological evidence of infectious disease is present, but an incidental cyst is seen in the right lobe.\"\n",
    "    ]\n",
    "    return text + \" \" + random.choice(fake_facts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c00c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def gpt_generate(original, instruction, model=\"gpt-4o\"):\n",
    "    prompt = f\"{instruction}\\n\\n{original}\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,  \n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95dcbb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chest X-ray does not reveal any acute abnormalities related to the heart or lungs.\n"
     ]
    }
   ],
   "source": [
    "original = \"Chest X-ray shows no acute cardiopulmonary abnormality.\"\n",
    "instruction = \"Paraphrase the following medical report without changing its clinical meaning:\"\n",
    "print(gpt_generate(original, instruction, model=\"gpt-3.5-turbo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc7b7e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20 / 1000 originals...\n",
      "Processed 40 / 1000 originals...\n",
      "Processed 60 / 1000 originals...\n",
      "Processed 80 / 1000 originals...\n",
      "Processed 100 / 1000 originals...\n",
      "Processed 120 / 1000 originals...\n",
      "Processed 140 / 1000 originals...\n",
      "Processed 160 / 1000 originals...\n",
      "Processed 180 / 1000 originals...\n",
      "Processed 200 / 1000 originals...\n",
      "Processed 220 / 1000 originals...\n",
      "Processed 240 / 1000 originals...\n",
      "Processed 260 / 1000 originals...\n",
      "Processed 280 / 1000 originals...\n",
      "Processed 300 / 1000 originals...\n",
      "Processed 320 / 1000 originals...\n",
      "Processed 340 / 1000 originals...\n",
      "Processed 360 / 1000 originals...\n",
      "Processed 380 / 1000 originals...\n",
      "Processed 400 / 1000 originals...\n",
      "Processed 420 / 1000 originals...\n",
      "Processed 440 / 1000 originals...\n",
      "Processed 460 / 1000 originals...\n",
      "Processed 480 / 1000 originals...\n",
      "Processed 500 / 1000 originals...\n",
      "Processed 520 / 1000 originals...\n",
      "Processed 540 / 1000 originals...\n",
      "Processed 560 / 1000 originals...\n",
      "Processed 580 / 1000 originals...\n",
      "Processed 600 / 1000 originals...\n",
      "Processed 620 / 1000 originals...\n",
      "Processed 640 / 1000 originals...\n",
      "Processed 660 / 1000 originals...\n",
      "Processed 680 / 1000 originals...\n",
      "Processed 700 / 1000 originals...\n",
      "Processed 720 / 1000 originals...\n",
      "Processed 740 / 1000 originals...\n",
      "Processed 760 / 1000 originals...\n",
      "Processed 780 / 1000 originals...\n",
      "Processed 800 / 1000 originals...\n",
      "Processed 820 / 1000 originals...\n",
      "Processed 840 / 1000 originals...\n",
      "Processed 860 / 1000 originals...\n",
      "Processed 880 / 1000 originals...\n",
      "Processed 900 / 1000 originals...\n",
      "Processed 920 / 1000 originals...\n",
      "Processed 940 / 1000 originals...\n",
      "Processed 960 / 1000 originals...\n",
      "Processed 980 / 1000 originals...\n",
      "Processed 1000 / 1000 originals...\n"
     ]
    }
   ],
   "source": [
    "synthetic_data = []\n",
    "for idx, row in df_sample.iterrows():\n",
    "    orig = row[\"Original\"]\n",
    "    meta = {\"Source\": row[\"Source\"], \"Specialty\": row[\"Specialty\"]}\n",
    "\n",
    "    # LLM生成的4类\n",
    "    for label in [\"Paraphrased\", \"Clinically Significant\", \"Noisy\", \"Neutral\"]:\n",
    "        try:\n",
    "            synthetic = gpt_generate(orig, INSTRUCTIONS[label], model=\"gpt-3.5-turbo\")\n",
    "        except Exception as e:\n",
    "            synthetic = f\"LLM_ERROR: {e}\"\n",
    "        synthetic_data.append({\n",
    "            \"Original\": orig,\n",
    "            \"Synthetic\": synthetic,\n",
    "            \"Label\": label,\n",
    "            **meta\n",
    "        })\n",
    "\n",
    "    # 脚本法2类\n",
    "    synthetic_data.append({\n",
    "        \"Original\": orig,\n",
    "        \"Synthetic\": script_omission(orig),\n",
    "        \"Label\": \"Omission\",\n",
    "        **meta\n",
    "    })\n",
    "    synthetic_data.append({\n",
    "        \"Original\": orig,\n",
    "        \"Synthetic\": script_hallucinated(orig),\n",
    "        \"Label\": \"Hallucinated\",\n",
    "        **meta\n",
    "    })\n",
    "\n",
    "    if (idx+1) % 20 == 0:  # 可以调整进度提示频率\n",
    "        print(f\"Processed {idx+1} / {len(df_sample)} originals...\")\n",
    "\n",
    "# 保存\n",
    "synthetic_df = pd.DataFrame(synthetic_data)\n",
    "synthetic_df.to_csv(\"synthetic_1000_gpt3.5-turbo.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee925ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 50/6000 [26:00<52:08:18, 31.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 50 条...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 77/6000 [39:46<54:55:07, 33.38s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. 读取API Key和初始化client\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "client_openai = OpenAI(api_key=openai_api_key)\n",
    "client_deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "client_gemini = genai.Client()\n",
    "\n",
    "# 2. 加载embedding模型\n",
    "tokenizer_bio = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "model_bio = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "tokenizer_clin = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model_clin = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# 3. 各模型的评分函数\n",
    "def llm_score(original, synthetic, client, model=\"gpt-4o\"):\n",
    "    prompt = (\n",
    "        \"You are a medical expert. How similar are the following two medical reports? Give a score from 0 to 10 and briefly explain your reasoning. Be precise and concise, avoid lengthy details.\\n\\n\"\n",
    "        f\"Report A (Original):\\n{original}\\n\\n\"\n",
    "        f\"Report B (Synthetic):\\n{synthetic}\"\n",
    "    )\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        content = resp.choices[0].message.content.strip()\n",
    "        match = re.search(r'(\\d+(\\.\\d+)?)(/10)?', content)\n",
    "        score = float(match.group(1)) if match else None\n",
    "        return score, content\n",
    "    except Exception as e:\n",
    "        return None, f\"{model}_ERROR: {e}\"\n",
    "\n",
    "def deepseek_score(original, synthetic, client):\n",
    "    # 用openai格式client，base_url已在初始化时指定\n",
    "    return llm_score(original, synthetic, client, model=\"deepseek-reasoner\")\n",
    "\n",
    "def gemini_score(original, synthetic, client):\n",
    "    prompt = (\n",
    "        \"You are a medical expert. How similar are the following two medical reports? Give a score from 0 to 10 and briefly explain your reasoning. Be precise and concise, avoid lengthy details.\\n\\n\"\n",
    "        f\"Report A (Original):\\n{original}\\n\\n\"\n",
    "        f\"Report B (Synthetic):\\n{synthetic}\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-pro\", contents=prompt\n",
    "        )\n",
    "        content = response.text\n",
    "        match = re.search(r'(\\d+(\\.\\d+)?)(/10)?', content)\n",
    "        score = float(match.group(1)) if match else None\n",
    "        return score, content\n",
    "    except Exception as e:\n",
    "        return None, f\"GEMINI_ERROR: {e}\"\n",
    "\n",
    "def bert_similarity(original, synthetic, tokenizer, model):\n",
    "    def get_emb(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "            emb = output.last_hidden_state.mean(dim=1).numpy()\n",
    "        return emb\n",
    "    emb1 = get_emb(original)\n",
    "    emb2 = get_emb(synthetic)\n",
    "    sim = cosine_similarity(emb1, emb2)[0][0]\n",
    "    return sim\n",
    "\n",
    "# 4. 读取数据\n",
    "synthetic_df = pd.read_csv(\"synthetic_1000_gpt3.5-turbo.csv\")\n",
    "\n",
    "results = []\n",
    "for idx, row in tqdm(synthetic_df.iterrows(), total=len(synthetic_df)):\n",
    "    orig, synth = row[\"Original\"], row[\"Synthetic\"]\n",
    "    meta = {k: row[k] for k in [\"Label\", \"Source\", \"Specialty\"]}\n",
    "    result = {\n",
    "        \"Original\": orig, \"Synthetic\": synth, **meta\n",
    "    }\n",
    "\n",
    "    # GPT-4o\n",
    "    score_4o, explain_4o = llm_score(orig, synth, client_openai, model=\"gpt-4o\")\n",
    "    result[\"GPT-4o_Score\"] = score_4o\n",
    "    result[\"GPT-4o_Explain\"] = explain_4o\n",
    "\n",
    "    # GPT-3.5-turbo\n",
    "    score_35, explain_35 = llm_score(orig, synth, client_openai, model=\"gpt-3.5-turbo\")\n",
    "    result[\"GPT-3.5_Score\"] = score_35\n",
    "    result[\"GPT-3.5_Explain\"] = explain_35\n",
    "\n",
    "    # DeepSeek\n",
    "    score_ds, explain_ds = deepseek_score(orig, synth, client_deepseek)\n",
    "    result[\"DeepSeek_Score\"] = score_ds\n",
    "    result[\"DeepSeek_Explain\"] = explain_ds\n",
    "\n",
    "    # Gemini\n",
    "    score_gem, explain_gem = gemini_score(orig, synth, client_gemini)\n",
    "    result[\"Gemini_Score\"] = score_gem\n",
    "    result[\"Gemini_Explain\"] = explain_gem\n",
    "\n",
    "    # BioBERT\n",
    "    result[\"BioBERT_sim\"] = bert_similarity(orig, synth, tokenizer_bio, model_bio)\n",
    "\n",
    "    # ClinicalBERT\n",
    "    result[\"ClinicalBERT_sim\"] = bert_similarity(orig, synth, tokenizer_clin, model_clin)\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    # 可选：定期保存，避免意外\n",
    "    if (idx+1) % 50 == 0:\n",
    "        pd.DataFrame(results).to_csv(\"synthetic_eval_partial.csv\", index=False)\n",
    "        print(f\"已保存 {idx+1} 条...\")\n",
    "\n",
    "# 最终保存\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"synthetic_eval_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
